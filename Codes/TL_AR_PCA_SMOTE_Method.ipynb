{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TL_AR_PCA_SMOTE_Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oef0BVF8C9Bd"
      },
      "source": [
        "#Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS04PSVNlOYA"
      },
      "source": [
        "import pandas as pd \n",
        "Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1_AR.csv\")\n",
        "Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2_AR.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNQypzzaDAIN"
      },
      "source": [
        "#Class Transformation\n",
        "We created a PCA-SMOTE transformation for the source and for the target datasets to transform the data into a new common feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFxHgDCTkWy_"
      },
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.decomposition import PCA\n",
        "np.random.seed(1)\n",
        "\n",
        "labels_S1 = Final_Data_S1.activity\n",
        "labels_S2 = Final_Data_S2.activity\n",
        "Data_S1 = Final_Data_S1.drop(['year_day', 'time', 'activity'], axis= 1)\n",
        "Data_S2 = Final_Data_S2.drop(['year_day', 'time', 'activity'], axis= 1)\n",
        "pca = PCA(n_components=7)\n",
        "principalComponents_S1 = pca.fit_transform(Data_S1)\n",
        "pca = PCA(n_components=6)\n",
        "principalComponents_S2 = pca.fit_transform(Data_S2)\n",
        "\n",
        "sm = SMOTE(random_state=1)\n",
        "principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n",
        "principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n",
        "\n",
        "principalComponents_S1 = pd.DataFrame(principalComponents_S1)\n",
        "principalComponents_S2 = pd.DataFrame(principalComponents_S2)\n",
        "labels_S1 = pd.Series(labels_S1)\n",
        "labels_S2 = pd.Series(labels_S2)\n",
        "\n",
        "principalComponents_S1.columns = ['pc_Source_' + str(i) for i in range(1, 8)]\n",
        "principalComponents_S2.columns = ['pc_Target_' + str(i) for i in range(1, 7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCC_NhkBDEzN"
      },
      "source": [
        "#Class Divergence_Calculation\n",
        "We calculated the divergence between features from source and target datasets using the JSD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUBKpaIzk9Q3"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from scipy.spatial import distance\n",
        "\n",
        "class Divergence_Calculation:\n",
        "\n",
        "  def add_target(self, Final_Data, labels):\n",
        "    Final_Data['activity'] = labels.values\n",
        "    return Final_Data\n",
        "\n",
        "  def prob_dist_divergence_initial(self, data, num_pc, name):\n",
        "\n",
        "    results = data\n",
        "    column = ['pc_'+ name + '_' + str(i) for i in range(1,num_pc+1)]\n",
        "    res = pd.DataFrame(data = np.zeros((5,1)), columns=['None'])\n",
        "    \n",
        "    for i in column:\n",
        "      table = pd.DataFrame(data = np.zeros((5,5)), \\\n",
        "                           columns= [i+'15.0', i+'60.0', i+'65.0', i+'70.0', i+'85.0'])\n",
        "      condit = pd.DataFrame(data = results.groupby('activity')[i])\n",
        "      for j in range(5):\n",
        "        x = condit.iloc[j,1]\n",
        "        table.iloc[:,j] = np.histogram(x, bins = 5)[0] / len(x)   \n",
        "      res = pd.concat([res, table], axis=1)\n",
        "\n",
        "    return res\n",
        "\n",
        "  \n",
        "  def jsd_final(self, X1_Source_init,X2_Target_init, Final_Data_Target):\n",
        "\n",
        "    target_prob = Final_Data_Target.activity.value_counts()/len(Final_Data_Target)\n",
        "    matrix = pd.DataFrame(data = np.zeros((Final_Data_Source.shape[1]-1, Final_Data_Target.shape[1]-1)))\n",
        "    matrix.columns = [i for i in Final_Data_Target.columns if i!= 'activity']\n",
        "    matrix.index = [i for i in Final_Data_Source.columns if i != 'activity']\n",
        "      \n",
        "    for i in matrix.columns:\n",
        "      for j in matrix.index:\n",
        "        matrix.loc[j,i] = sum([target_prob.loc[target_prob.index == k].\\\n",
        "          values[0]*distance.jensenshannon(X1_Source_init[j+str(k)], X2_Target_init[i+str(k)], 2.0) for k in target_prob.index])\n",
        "    return matrix\n",
        "\n",
        "#script\n",
        "divergence_calculation = Divergence_Calculation()\n",
        "Final_Data_Source = divergence_calculation.add_target(principalComponents_S1, labels_S1)\n",
        "Final_Data_Target = divergence_calculation.add_target(principalComponents_S2, labels_S2)\n",
        "\n",
        "X1_Source_init = divergence_calculation.prob_dist_divergence_initial(Final_Data_Source, 7, 'Source')\n",
        "X2_Target_init = divergence_calculation.prob_dist_divergence_initial(Final_Data_Target, 6, 'Target')\n",
        "\n",
        "divergence_matrix = divergence_calculation.jsd_final(X1_Source_init,X2_Target_init, Final_Data_Target)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NyGtkuDPHs"
      },
      "source": [
        "#Class PreMapping\n",
        "We used Thresholidng to avoid negative transfer and we created the Preferences lists for the mapping procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmEE1-bGp2Bf"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "class PreMapping:\n",
        "\n",
        "  def threshold_select(self, divergence_matrix, threshold):\n",
        "    divergence_matrix_bool = divergence_matrix > threshold\n",
        "    indx = []\n",
        "\n",
        "    for j in range(divergence_matrix.shape[1]):\n",
        "      Test = True\n",
        "      for i in range(len(divergence_matrix)):\n",
        "        if divergence_matrix_bool.iloc[i,j] == False:\n",
        "          Test = False\n",
        "      if Test == True:\n",
        "        indx.append(j)\n",
        "\n",
        "    new_indx = [i for i in range(divergence_matrix.shape[1]) if i not in indx]\n",
        "    divergence_matrix = divergence_matrix.iloc[:,new_indx]\n",
        "    return divergence_matrix\n",
        "\n",
        "  def preferences(self, divergence_matrix):\n",
        "    Source_features = [i for i in divergence_matrix.index]\n",
        "    Target_features = [i for i in divergence_matrix.columns]\n",
        "\n",
        "    priority_source = {i:list(divergence_matrix.loc[i,:].sort_values().index) for i in Source_features}\n",
        "    priority_target = {i:list(divergence_matrix.loc[:,i].sort_values().index) for i in Target_features}\n",
        "\n",
        "    return priority_source, priority_target\n",
        "\n",
        "#script\n",
        "preMapping = PreMapping()\n",
        "#divergence_matrix = preMapping.threshold_select(divergence_matrix, 0.35)\n",
        "priority_source, priority_target = preMapping.preferences(divergence_matrix)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZBYLQSwDSRu"
      },
      "source": [
        "#Class Mapping\n",
        "We applied the Gale-Shapley Algorithm to map features from both domains based on their divergence values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ZIfzNvp_vZ"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from collections import defaultdict\n",
        "class Mapping:\n",
        "\n",
        "    def __init__(self, men, women):\n",
        "        '''\n",
        "        Constructs a Matcher instance.\n",
        "        Takes a dict of men's spousal preferences, `men`,\n",
        "        and a dict of women's spousal preferences, `women`.\n",
        "        '''\n",
        "        self.M = men\n",
        "        self.W = women\n",
        "        self.wives = {}\n",
        "        self.pairs = []\n",
        "\n",
        "        # we index spousal preferences at initialization \n",
        "        # to avoid expensive lookups when matching\n",
        "        self.mrank = defaultdict(dict)  # `mrank[m][w]` is m's ranking of w\n",
        "        self.wrank = defaultdict(dict)  # `wrank[w][m]` is w's ranking of m\n",
        "\n",
        "        for m, prefs in men.items():\n",
        "            for i, w in enumerate(prefs):\n",
        "                self.mrank[m][w] = i\n",
        "\n",
        "        for w, prefs in women.items():\n",
        "            for i, m in enumerate(prefs):\n",
        "                self.wrank[w][m] = i\n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.match()\n",
        "\n",
        "    def prefers(self, w, m, h):\n",
        "        '''Test whether w prefers m over h.'''\n",
        "        return self.wrank[w][m] < self.wrank[w][h]\n",
        "\n",
        "    def after(self, m, w):\n",
        "        '''Return the woman favored by m after w.'''\n",
        "        i = self.mrank[m][w] + 1    # index of woman following w in list of prefs\n",
        "        return self.M[m][i]\n",
        "\n",
        "    def match(self, men=None, next=None, wives=None):\n",
        "        '''\n",
        "        Try to match all men with their next preferred spouse.\n",
        "        \n",
        "        '''\n",
        "        if men is None: \n",
        "            men = self.M.keys()         # get the complete list of men\n",
        "        if next is None: \n",
        "            # if not defined, map each man to their first preference\n",
        "            next = dict((m, rank[0]) for m, rank in self.M.items()) \n",
        "        if wives is None: \n",
        "            wives = {}                  # mapping from women to current spouse\n",
        "        if not len(men): \n",
        "            self.pairs = [(h, w) for w, h in wives.items()]\n",
        "            self.wives = wives\n",
        "            return wives\n",
        "        m, men = list(men)[0], list(men)[1:]\n",
        "        w = next[m]                     # next woman for m to propose to\n",
        "        next[m] = self.after(m, w)      # woman after w in m's list of prefs\n",
        "        if w in wives:\n",
        "            h = wives[w]                # current husband\n",
        "            if self.prefers(w, m, h):\n",
        "                men.append(h)           # husband becomes available again\n",
        "                wives[w] = m            # w becomes wife of m\n",
        "            else:\n",
        "                men.append(m)           # m remains unmarried\n",
        "        else:\n",
        "            wives[w] = m                # w becomes wife of m\n",
        "        return self.match(men, next, wives)\n",
        "\n",
        "    def map_source(self, Final_Data_Source, Final_Data_Target, Final_Match):\n",
        "\n",
        "      Final_Data_Source = Final_Data_Source.rename(columns=Final_Match)\n",
        "      Final_Data_Source = Final_Data_Source[Final_Data_Target.columns]\n",
        "      return Final_Data_Source\n",
        "\n",
        "#script\n",
        "mapping = Mapping(priority_target, priority_source)\n",
        "Final_Match = mapping.match()\n",
        "Final_Data_Target = Final_Data_Target[list(Final_Match.values()) + ['activity']]\n",
        "Final_Data_Source = mapping.map_source(Final_Data_Source, Final_Data_Target, Final_Match)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4KXbMgDVN1"
      },
      "source": [
        "#Class PostMapping\n",
        "We added target data in the training to enhance the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2gjuL9vqCEg"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "class PostMapping:\n",
        "\n",
        "  def shuffle_data(self, Final_Data):\n",
        "    Final_Data = Final_Data.sample(frac = 1)\n",
        "    return Final_Data\n",
        "\n",
        "  def rearrage_data(self, Final_Data_Source, Final_Data_Target, number_rows):\n",
        "    add_data = Final_Data_Target.iloc[:number_rows,:]\n",
        "    Final_Data_Target = Final_Data_Target.iloc[number_rows:,:]\n",
        "    frames = [Final_Data_Source, add_data]\n",
        "    Final_Data_Source = pd.concat(frames)\n",
        "\n",
        "    return Final_Data_Source, Final_Data_Target\n",
        "\n",
        "\n",
        "#script\n",
        "postMapping = PostMapping()\n",
        "Final_Data_Source = postMapping.shuffle_data(Final_Data_Source)\n",
        "Final_Data_Target = postMapping.shuffle_data(Final_Data_Target)\n",
        "Data_Target = Final_Data_Target\n",
        "# Target data added in the training\n",
        "# 0 days = 0\n",
        "# 2 days = 922\n",
        "# 4 days = 1844\n",
        "# 6 days = 2766\n",
        "# 8 days = 3688\n",
        "# 10 days = 4610\n",
        "Final_Data_Source, Final_Data_Target = postMapping.rearrage_data(Final_Data_Source, Final_Data_Target, 4610)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZZUvvarCzTn"
      },
      "source": [
        "#Class Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz-_int3qOqv",
        "outputId": "d6fc5eda-8581-4d2c-d4b2-e30be03b2c8d"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "class Modeling:\n",
        "\n",
        "  def split_train_test(self, Final_Data_Source, Final_Data_Target):\n",
        "    X_train = Final_Data_Source.drop(['activity'], axis=1)\n",
        "    y_train = Final_Data_Source['activity']\n",
        "    X_test = Final_Data_Target.drop(['activity'], axis=1)\n",
        "    y_test = Final_Data_Target['activity']\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def split_train_val(self, X, y):    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "  def pipelines_def(self):\n",
        "    pipelines = []\n",
        "    params = []\n",
        "    names = []\n",
        "    #Notice that we tried to balance the data via using the clf__class_weight parameter in the models\n",
        "\n",
        "    pipelines.append(Pipeline([('clf', DecisionTreeClassifier())])) ## DecisionTreeClassifier\n",
        "    params.append({'clf__max_features': [None], 'clf__min_samples_split': [2], 'clf__min_samples_leaf':[1],\n",
        "                  'clf__class_weight': ['balanced']})\n",
        "    names.append('DecisionTreeClassifier') \n",
        "\n",
        "\n",
        "\n",
        "    return pipelines, params, names\n",
        "\n",
        "  def model(self, pipeline, param, name, X, y):    \n",
        "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=32)\n",
        "\n",
        "    grid_obj = GridSearchCV(estimator=pipeline, param_grid=param, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
        "    grid_obj.fit(X,y)  \n",
        "\n",
        "    print(name, 'F1-measure:', grid_obj.best_score_)\n",
        "    estimator = grid_obj.best_estimator_\n",
        "    estimator.fit(X,y) # train on all training dataset\n",
        "    return estimator \n",
        "\n",
        "\n",
        "  def estimators(self, pipelines, params, names,  X_train, y_train):\n",
        "    estimators = []\n",
        "    for idx in range(0,len(pipelines)):    \n",
        "        estimators.append(self.model(pipelines[idx], params[idx], names[idx], X_train, y_train))\n",
        "    return estimators\n",
        "\n",
        "\n",
        "  def evaluate_models(self,estimators, names, X_test, y_test):\n",
        "   \n",
        "    for idx, estimator in enumerate(estimators):\n",
        "      print('\\nPerformance of', names[idx])\n",
        "      y_pred = estimator.predict(X_test)       \n",
        "      print('\\nConfusion matrix\\n', confusion_matrix(y_test, y_pred), '\\n')    \n",
        "      print('F1-measure', f1_score(y_test, y_pred, average='micro'), '\\n')\n",
        "\n",
        "      precision, recall, fscore, support = score(y_test, y_pred)\n",
        "\n",
        "      print('precision: {}'.format(precision))\n",
        "      print('recall: {}'.format(recall))\n",
        "      print('fscore: {}'.format(fscore))\n",
        "      print('support: {}'.format(support))\n",
        "\n",
        "    return\n",
        "\n",
        "#script\n",
        "modeling = Modeling()\n",
        "\n",
        "X_train, y_train, X_test, y_test = modeling.split_train_test(Final_Data_Source, Final_Data_Target)\n",
        "X_test_T, y_test_T, X_test_T, y_test_T = modeling.split_train_test(Data_Target, Data_Target)\n",
        "X_train_S,  X_test_S, y_train_S, y_test_S = modeling.split_train_val(X_test_T, y_test_T                                                                  \n",
        "                                                                     )\n",
        "pipelines, params, names = modeling.pipelines_def()\n",
        "print('Source Training results_without TL')\n",
        "estimators = modeling.estimators(pipelines, params, names, X_train_S, y_train_S)\n",
        "print('Source Testing results_without TL')\n",
        "modeling.evaluate_models(estimators, names, X_test_S, y_test_S)\n",
        "\n",
        "\n",
        "print('Training results_TL')\n",
        "estimators = modeling.estimators(pipelines, params, names, X_train, y_train)\n",
        "print('Testing results_TL')\n",
        "modeling.evaluate_models(estimators, names, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Training results_without TL\n",
            "DecisionTreeClassifier F1-measure: 0.896197731687695\n",
            "Source Testing results_without TL\n",
            "\n",
            "Performance of DecisionTreeClassifier\n",
            "\n",
            "Confusion matrix\n",
            " [[309  24  15  13   6]\n",
            " [ 14 387   1   0   2]\n",
            " [ 15  19 355   1   9]\n",
            " [ 10   1   1 361   0]\n",
            " [  2   0  63   2 327]] \n",
            "\n",
            "F1-measure 0.8977800722767165 \n",
            "\n",
            "precision: [0.88285714 0.89791183 0.81609195 0.95755968 0.9505814 ]\n",
            "recall: [0.84196185 0.95792079 0.88972431 0.96782842 0.82994924]\n",
            "fscore: [0.86192469 0.92694611 0.85131894 0.96266667 0.88617886]\n",
            "support: [367 404 399 373 394]\n",
            "Training results_TL\n",
            "DecisionTreeClassifier F1-measure: 0.9176190476190476\n",
            "Testing results_TL\n",
            "\n",
            "Performance of DecisionTreeClassifier\n",
            "\n",
            "Confusion matrix\n",
            " [[327  25   3  10  11]\n",
            " [ 10 359   4   0   2]\n",
            " [ 16  14 355   2   2]\n",
            " [  8   0   0 336   0]\n",
            " [  4   0  60   1 296]] \n",
            "\n",
            "F1-measure 0.9067750677506775 \n",
            "\n",
            "precision: [0.89589041 0.90201005 0.84123223 0.96275072 0.95176849]\n",
            "recall: [0.86968085 0.95733333 0.9125964  0.97674419 0.8199446 ]\n",
            "fscore: [0.88259109 0.92884864 0.87546239 0.96969697 0.88095238]\n",
            "support: [376 375 389 344 361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APFUF3X1qR_X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}