{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TL_AR_PCA_Like_Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57YHYxWhHtM3"
      },
      "source": [
        "#Class Preprocessing\n",
        "\n",
        "This cell contains the preparation of the orginal data. Please skip it because the data is already prepared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVChW-JflTyL"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from scipy import io\n",
        "\n",
        "class Preprocessing:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.name = 'Preprocessing'\n",
        "\n",
        "  def loading(self, path_activities_1, path_activities_2, path_sensors_1, path_sensors_2):\n",
        "    activities_data = io.loadmat(path_activities_1)\n",
        "    activities_data_S1 = pd.DataFrame(activities_data['activities_data'] ,\\\n",
        "                                      columns = ['year_day', 'week_day', 'start_time', 'activity_code', 'subcategory_index', 'end_time', 'indicator'])\n",
        "\n",
        "    activities_data = io.loadmat(path_activities_2)\n",
        "    activities_data_S2 = pd.DataFrame(activities_data['activities_data'] ,\\\n",
        "                                      columns = ['year_day', 'week_day', 'start_time', 'activity_code', 'subcategory_index', 'end_time', 'indicator'])\n",
        "\n",
        "\n",
        "    sensors_data = io.loadmat(path_sensors_1)\n",
        "    sensors_data_S1 = pd.DataFrame(sensors_data['sensor_data'])\n",
        "    sensors_data_S1.columns = ['year_day', 'week_day', 'activation' ,'deactivation', 'interval', 'sensor_id', 'location' ,'type']\n",
        "\n",
        "    sensors_data = io.loadmat(path_sensors_2)\n",
        "    sensors_data_S2 = pd.DataFrame(sensors_data['sensor_data'])\n",
        "    sensors_data_S2.columns = ['year_day', 'week_day', 'activation' ,'deactivation', 'interval', 'sensor_id', 'location' ,'type']\n",
        "\n",
        "    return activities_data_S1, activities_data_S2, sensors_data_S1, sensors_data_S2\n",
        "\n",
        "  def recognize_activity(self, t): \n",
        "    activity = -1 \n",
        "    for inter_key in activites_interval.keys() :\n",
        "      start =inter_key[0] \n",
        "      end =  inter_key[1] \n",
        "      if t < end and t > start  : \n",
        "        activity  = activites_interval[inter_key] \n",
        "        break \n",
        "    return activity\n",
        "\n",
        "  def is_sensor_active(self, t): \n",
        "    act = 0 \n",
        "    for i in range(len(start_int)) : \n",
        "      if t < end_int[i] and t > start_int[i] : \n",
        "        act = 1 \n",
        "        break \n",
        "    return act \n",
        "\n",
        "  def data_processing(self, activities_data, sensors_data):\n",
        "    global activites_interval\n",
        "    global start_int\n",
        "    global end_int\n",
        "    sensor_id_t = sensors_data.sensor_id.unique()\n",
        "    year_day_t = sensors_data.year_day.unique()\n",
        "    for i in year_day_t[0:1]:\n",
        "      data = pd.DataFrame(data = np.arange(0,3600*24,30) ) \n",
        "      data.columns = ['time']\n",
        "      activities_data_t = activities_data[activities_data['year_day']==i]\n",
        "      keys = activities_data_t['activity_code'].values \n",
        "      start = activities_data_t['start_time'].values\n",
        "      end = activities_data_t['end_time'].values \n",
        "      activites_interval = {}\n",
        "      for id , activity in enumerate(keys) : \n",
        "        activites_interval[(start[id] , end[id])] = activity\n",
        "      data['year_day'] = pd.DataFrame(data = [i]*2880 ) \n",
        "      recognize_activity = self.recognize_activity \n",
        "      data['activity'] = data['time'].apply(recognize_activity)\n",
        "      sensors_data_t = sensors_data[sensors_data['year_day']==i]\n",
        "      for j in sensor_id_t:\n",
        "        sensors_data_t_n = sensors_data_t[sensors_data_t['sensor_id']==j]\n",
        "        start_int = sensors_data_t_n.activation.values \n",
        "        end_int = sensors_data_t_n.deactivation.values \n",
        "        is_sensor_active = self.is_sensor_active\n",
        "        data['sensor_id_'+str(j)] = data.time.apply(is_sensor_active)\n",
        "    \n",
        "      Final_Data = data\n",
        "\n",
        "    for i in year_day_t[1:]:\n",
        "      data = pd.DataFrame(data = np.arange(0,3600*24,30) ) \n",
        "      data.columns = ['time']\n",
        "      activities_data_t = activities_data[activities_data['year_day']==i]\n",
        "      keys = activities_data_t['activity_code'].values \n",
        "      start = activities_data_t['start_time'].values\n",
        "      end = activities_data_t['end_time'].values \n",
        "      activites_interval = {}\n",
        "      for id , activity in enumerate(keys) : \n",
        "        activites_interval[(start[id] , end[id])] = activity\n",
        "      data['year_day'] = pd.DataFrame(data = [i]*2880 )\n",
        "      recognize_activity = self.recognize_activity \n",
        "      data['activity'] = data['time'].apply(recognize_activity)\n",
        "\n",
        "      sensors_data_t = sensors_data[sensors_data['year_day']==i]\n",
        "      for j in sensor_id_t:\n",
        "        sensors_data_t_n = sensors_data_t[sensors_data_t['sensor_id']==j]\n",
        "        start_int = sensors_data_t_n.activation.values \n",
        "        end_int = sensors_data_t_n.deactivation.values \n",
        "        is_sensor_active = self.is_sensor_active\n",
        "        data['sensor_id_'+str(j)] = data.time.apply(is_sensor_active)\n",
        "      \n",
        "      new_data = [Final_Data,data]\n",
        "      Final_Data = pd.concat(new_data, ignore_index= True)\n",
        "    return Final_Data\n",
        "\n",
        "  def change_labels(self, Final_Data):\n",
        "    Final_Data.loc[Final_Data[Final_Data.activity.\\\n",
        "                                    isin([20.0,25.0,105.0,100.0,80.0,30.0,5.0,140.0,135.0,40.0,75.0])].index,'activity']=-1\n",
        "\n",
        "    return Final_Data\n",
        "\n",
        "  def remove_rows(self, Final_Data):\n",
        "    Final_Data = Final_Data[Final_Data.activity != -1.0]\n",
        "    return Final_Data\n",
        "\n",
        "  def align_features(self, data_aligned, data_to_align):\n",
        "    data_to_align = data_to_align.iloc[:,:data_aligned.shape[1]]\n",
        "    return data_to_align\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#script\n",
        "path_activities_1 = 'activities_data1.mat'\n",
        "path_activities_2 = 'activities_data2.mat'\n",
        "path_sensors_1 = 'sensor_data1.mat'\n",
        "path_sensors_2 = 'sensor_data2.mat'\n",
        "\n",
        "preprocessing = Preprocessing()\n",
        "activities_data_S2, activities_data_S1, sensors_data_S2, sensors_data_S1 = preprocessing.loading(path_activities_1,\\\n",
        "                                                                                                 path_activities_2, path_sensors_1, path_sensors_2)\n",
        "Final_Data_S1 = preprocessing.data_processing(activities_data_S1, sensors_data_S1)\n",
        "Final_Data_S2 = preprocessing.data_processing(activities_data_S2, sensors_data_S2)\n",
        "\n",
        "Final_Data_S1 = preprocessing.change_labels(Final_Data_S1)\n",
        "Final_Data_S2 = preprocessing.change_labels(Final_Data_S2)\n",
        "\n",
        "Final_Data_S1 = preprocessing.remove_rows(Final_Data_S1)\n",
        "Final_Data_S2 = preprocessing.remove_rows(Final_Data_S2)\n",
        "\n",
        "#Final_Data_S2 = preprocessing.align_features(Final_Data_S1, Final_Data_S2)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ineR15q-3JyB"
      },
      "source": [
        "#Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnwFoiiSg2Rw"
      },
      "source": [
        "import pandas as pd \n",
        "Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1_AR.csv\")\n",
        "Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2_AR.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEfcphyrg8sd"
      },
      "source": [
        "#Class Transformation\n",
        "We created a PCA-like transformation for the source and for the target datasets to transform the data into a new common feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAaXBjfMHmu2"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "np.random.seed(2)\n",
        "\n",
        "class Transformation:\n",
        "\n",
        "  def remove_features(self, Final_Data):\n",
        "    Final_Data = Final_Data.drop(['time', 'year_day'], axis = 1)\n",
        "    return Final_Data\n",
        "\n",
        "  def centered_data(self, data):\n",
        "    for i in range(1,data.shape[1]):\n",
        "      mean = data.iloc[:,i].mean()\n",
        "      data.iloc[:,i] = data.iloc[:,i] - mean\n",
        "    return data\n",
        "\n",
        "  def Transoform_Data(self, data, X, n_pca, name, number_activity):\n",
        "\n",
        "    data_prob = data.activity.value_counts()/len(data)\n",
        "    indx_prob = {i:data_prob[i] for i in data_prob.index}\n",
        "\n",
        "    initial_activity = pd.DataFrame(data = data.groupby('activity').cov().iloc[0:data.shape[1]-1]).index[0][0]\n",
        "    cov = (data.groupby('activity').cov().iloc[0:data.shape[1]-1])*indx_prob[initial_activity]\n",
        "\n",
        "    for i in range(data.shape[1]-1,number_activity*(data.shape[1]-1),data.shape[1]-1):\n",
        "      next_activity = pd.DataFrame(data = data.groupby('activity').cov().iloc[i:i+data.shape[1]-1]).index[0][0]\n",
        "      cov += (data.groupby('activity').cov().iloc[i:i+data.shape[1]-1].values)*indx_prob[next_activity]\n",
        "\n",
        "    cov = pd.DataFrame(data = cov)\n",
        "    eigenValues, eigenVectors = np.linalg.eig(cov)\n",
        "    idx = eigenValues.argsort()[::-1]   \n",
        "    eigenValues = eigenValues[idx]\n",
        "    eigenVectors = eigenVectors[:,idx]\n",
        "\n",
        "    \n",
        "    Cov_Matrix = pd.DataFrame(data = eigenVectors).iloc[:,:n_pca]\n",
        "    Columns = ['pc_'+name + '_' + str(i) for i in range(1,n_pca+1)]\n",
        "    Cov_Matrix.columns = Columns\n",
        "\n",
        "\n",
        "    normalized_Cov_Matrix=(Cov_Matrix-Cov_Matrix.min())/(Cov_Matrix.max()-Cov_Matrix.min())\n",
        "    Transform_Matrix = Cov_Matrix.transpose()\n",
        "    X = X.drop(['activity'], axis = 1).transpose()\n",
        "    Final_Data = np.dot(Transform_Matrix, X)\n",
        "    Final_Data = pd.DataFrame(data = Final_Data.transpose(), columns= Columns)\n",
        "\n",
        "    return Final_Data\n",
        "\n",
        "#script\n",
        "transformation = Transformation()\n",
        "Cov_Data_S1 = transformation.remove_features(Final_Data_S1)\n",
        "Cov_Data_S2 = transformation.remove_features(Final_Data_S2)\n",
        "\n",
        "#Cov_Data_S1 = transformation.centered_data(Cov_Data_S1)\n",
        "#Cov_Data_S2 = transformation.centered_data(Cov_Data_S2)\n",
        "\n",
        "Final_Data_Source = transformation.Transoform_Data(Cov_Data_S1, Cov_Data_S1, 10, 'Source',5)\n",
        "Final_Data_Target = transformation.Transoform_Data(Cov_Data_S2, Cov_Data_S2, 9, 'Target',5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qge1k_JQlfwW"
      },
      "source": [
        "#Class Divergence_Calculation\n",
        "We calculated the divergence between features from source and target datasets using the JSD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aigkE6GjCeb"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from scipy.spatial import distance\n",
        "\n",
        "class Divergence_Calculation:\n",
        "\n",
        "  def add_target(self, Final_Data, Cov_Data):\n",
        "    Final_Data['activity'] = Cov_Data.activity.values\n",
        "    return Final_Data\n",
        "\n",
        "  def prob_dist_divergence_initial(self, data, num_pc, name):\n",
        "\n",
        "    results = data\n",
        "    column = ['pc_'+ name + '_' + str(i) for i in range(1,num_pc+1)]\n",
        "    res = pd.DataFrame(data = np.zeros((5,1)), columns=['None'])\n",
        "    \n",
        "    for i in column:\n",
        "      table = pd.DataFrame(data = np.zeros((5,5)), \\\n",
        "                           columns= [i+'15.0', i+'60.0', i+'65.0', i+'70.0', i+'85.0'])\n",
        "      condit = pd.DataFrame(data = results.groupby('activity')[i])\n",
        "      for j in range(5):\n",
        "        x = condit.iloc[j,1]\n",
        "        table.iloc[:,j] = np.histogram(x, bins = 5)[0] / len(x)   \n",
        "      res = pd.concat([res, table], axis=1)\n",
        "\n",
        "    return res\n",
        "\n",
        "  \n",
        "  def jsd_final(self, X1_Source_init,X2_Target_init, Final_Data_Target):\n",
        "\n",
        "    target_prob = Final_Data_Target.activity.value_counts()/len(Final_Data_Target)\n",
        "    matrix = pd.DataFrame(data = np.zeros((Final_Data_Source.shape[1]-1, Final_Data_Target.shape[1]-1)))\n",
        "    matrix.columns = [i for i in Final_Data_Target.columns if i!= 'activity']\n",
        "    matrix.index = [i for i in Final_Data_Source.columns if i != 'activity']\n",
        "      \n",
        "    for i in matrix.columns:\n",
        "      for j in matrix.index:\n",
        "        matrix.loc[j,i] = sum([target_prob.loc[target_prob.index == k].\\\n",
        "          values[0]*distance.jensenshannon(X1_Source_init[j+str(k)], X2_Target_init[i+str(k)], 2.0) for k in target_prob.index])\n",
        "    return matrix\n",
        "\n",
        "#script\n",
        "divergence_calculation = Divergence_Calculation()\n",
        "Final_Data_Source = divergence_calculation.add_target(Final_Data_Source, Cov_Data_S1)\n",
        "Final_Data_Target = divergence_calculation.add_target(Final_Data_Target, Cov_Data_S2)\n",
        "\n",
        "X1_Source_init = divergence_calculation.prob_dist_divergence_initial(Final_Data_Source, 10, 'Source')\n",
        "X2_Target_init = divergence_calculation.prob_dist_divergence_initial(Final_Data_Target, 9, 'Target')\n",
        "\n",
        "divergence_matrix = divergence_calculation.jsd_final(X1_Source_init,X2_Target_init, Final_Data_Target)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5wPEEjpkP7"
      },
      "source": [
        "#Class PreMapping\n",
        "We used Thresholidng to avoid negative transfer and we created the Preferences lists for the mapping procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EI6P7Tnn1Cx"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "class PreMapping:\n",
        "\n",
        "  def threshold_select(self, divergence_matrix, threshold):\n",
        "    divergence_matrix_bool = divergence_matrix > threshold\n",
        "    indx = []\n",
        "\n",
        "    for j in range(divergence_matrix.shape[1]):\n",
        "      Test = True\n",
        "      for i in range(len(divergence_matrix)):\n",
        "        if divergence_matrix_bool.iloc[i,j] == False:\n",
        "          Test = False\n",
        "      if Test == True:\n",
        "        indx.append(j)\n",
        "\n",
        "    new_indx = [i for i in range(divergence_matrix.shape[1]) if i not in indx]\n",
        "    divergence_matrix = divergence_matrix.iloc[:,new_indx]\n",
        "    return divergence_matrix\n",
        "\n",
        "  def preferences(self, divergence_matrix):\n",
        "    Source_features = [i for i in divergence_matrix.index]\n",
        "    Target_features = [i for i in divergence_matrix.columns]\n",
        "\n",
        "    priority_source = {i:list(divergence_matrix.loc[i,:].sort_values().index) for i in Source_features}\n",
        "    priority_target = {i:list(divergence_matrix.loc[:,i].sort_values().index) for i in Target_features}\n",
        "\n",
        "    return priority_source, priority_target\n",
        "\n",
        "#script\n",
        "preMapping = PreMapping()\n",
        "#divergence_matrix = preMapping.threshold_select(divergence_matrix, 0.32)\n",
        "priority_source, priority_target = preMapping.preferences(divergence_matrix)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_njtdPCkKvXP"
      },
      "source": [
        "#Class Mapping\n",
        "We applied the Gale-Shapley Algorithm to map features from both domains based on their divergence values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWFxACdHJq_d"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from collections import defaultdict\n",
        "class Mapping:\n",
        "\n",
        "    def __init__(self, men, women):\n",
        "        '''\n",
        "        Constructs a Matcher instance.\n",
        "        Takes a dict of men's spousal preferences, `men`,\n",
        "        and a dict of women's spousal preferences, `women`.\n",
        "        '''\n",
        "        self.M = men\n",
        "        self.W = women\n",
        "        self.wives = {}\n",
        "        self.pairs = []\n",
        "\n",
        "        # we index spousal preferences at initialization \n",
        "        # to avoid expensive lookups when matching\n",
        "        self.mrank = defaultdict(dict)  # `mrank[m][w]` is m's ranking of w\n",
        "        self.wrank = defaultdict(dict)  # `wrank[w][m]` is w's ranking of m\n",
        "\n",
        "        for m, prefs in men.items():\n",
        "            for i, w in enumerate(prefs):\n",
        "                self.mrank[m][w] = i\n",
        "\n",
        "        for w, prefs in women.items():\n",
        "            for i, m in enumerate(prefs):\n",
        "                self.wrank[w][m] = i\n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.match()\n",
        "\n",
        "    def prefers(self, w, m, h):\n",
        "        '''Test whether w prefers m over h.'''\n",
        "        return self.wrank[w][m] < self.wrank[w][h]\n",
        "\n",
        "    def after(self, m, w):\n",
        "        '''Return the woman favored by m after w.'''\n",
        "        i = self.mrank[m][w] + 1    # index of woman following w in list of prefs\n",
        "        return self.M[m][i]\n",
        "\n",
        "    def match(self, men=None, next=None, wives=None):\n",
        "        '''\n",
        "        Try to match all men with their next preferred spouse.\n",
        "        \n",
        "        '''\n",
        "        if men is None: \n",
        "            men = self.M.keys()         # get the complete list of men\n",
        "        if next is None: \n",
        "            # if not defined, map each man to their first preference\n",
        "            next = dict((m, rank[0]) for m, rank in self.M.items()) \n",
        "        if wives is None: \n",
        "            wives = {}                  # mapping from women to current spouse\n",
        "        if not len(men): \n",
        "            self.pairs = [(h, w) for w, h in wives.items()]\n",
        "            self.wives = wives\n",
        "            return wives\n",
        "        m, men = list(men)[0], list(men)[1:]\n",
        "        w = next[m]                     # next woman for m to propose to\n",
        "        next[m] = self.after(m, w)      # woman after w in m's list of prefs\n",
        "        if w in wives:\n",
        "            h = wives[w]                # current husband\n",
        "            if self.prefers(w, m, h):\n",
        "                men.append(h)           # husband becomes available again\n",
        "                wives[w] = m            # w becomes wife of m\n",
        "            else:\n",
        "                men.append(m)           # m remains unmarried\n",
        "        else:\n",
        "            wives[w] = m                # w becomes wife of m\n",
        "        return self.match(men, next, wives)\n",
        "\n",
        "    def map_source(self, Final_Data_Source, Final_Data_Target, Final_Match):\n",
        "\n",
        "      Final_Data_Source = Final_Data_Source.rename(columns=Final_Match)\n",
        "      Final_Data_Source = Final_Data_Source[Final_Data_Target.columns]\n",
        "      return Final_Data_Source\n",
        "\n",
        "#script\n",
        "mapping = Mapping(priority_target, priority_source)\n",
        "Final_Match = mapping.match()\n",
        "Final_Data_Target = Final_Data_Target[list(Final_Match.values()) + ['activity']]\n",
        "Final_Data_Source = mapping.map_source(Final_Data_Source, Final_Data_Target, Final_Match)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJQxA1KvDH0V"
      },
      "source": [
        "#Class PostMapping\n",
        "We added target data in the training to enhance the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3tKPwoJDOaD"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "class PostMapping:\n",
        "\n",
        "  def shuffle_data(self, Final_Data):\n",
        "    Final_Data = Final_Data.sample(frac = 1)\n",
        "    return Final_Data\n",
        "\n",
        "  def rearrage_data(self, Final_Data_Source, Final_Data_Target, number_rows):\n",
        "    add_data = Final_Data_Target.iloc[:number_rows,:]\n",
        "    Final_Data_Target = Final_Data_Target.iloc[number_rows:,:]\n",
        "    frames = [Final_Data_Source, add_data]\n",
        "    Final_Data_Source = pd.concat(frames)\n",
        "\n",
        "    return Final_Data_Source, Final_Data_Target\n",
        "\n",
        "\n",
        "#script\n",
        "postMapping = PostMapping()\n",
        "Final_Data_Source = postMapping.shuffle_data(Final_Data_Source)\n",
        "Final_Data_Target = postMapping.shuffle_data(Final_Data_Target)\n",
        "Data_Target = Final_Data_Target\n",
        "# Target data added in the training\n",
        "# 0 days = 0\n",
        "# 2 days = 488\n",
        "# 4 days = 975\n",
        "# 6 days = 1462\n",
        "# 8 days = 1950\n",
        "# 10 days = 2440\n",
        "\n",
        "Final_Data_Source, Final_Data_Target = postMapping.rearrage_data(Final_Data_Source, Final_Data_Target, 2440 )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnPN55PtN2d-"
      },
      "source": [
        "#Class Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EFDpuRBNMEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117eaa4f-6857-442e-a041-0698cd284ee6"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "class Modeling:\n",
        "\n",
        "  def split_train_test(self, Final_Data_Source, Final_Data_Target):\n",
        "    X_train = Final_Data_Source.drop(['activity'], axis=1)\n",
        "    y_train = Final_Data_Source['activity']\n",
        "    X_test = Final_Data_Target.drop(['activity'], axis=1)\n",
        "    y_test = Final_Data_Target['activity']\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def split_train_val(self, X, y):    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "  def pipelines_def(self):\n",
        "    pipelines = []\n",
        "    params = []\n",
        "    names = []\n",
        "    #Notice that we tried to balance the data via using the clf__class_weight parameter in the models\n",
        "\n",
        "   \n",
        "    pipelines.append(Pipeline([('clf', DecisionTreeClassifier())])) ## DecisionTreeClassifier\n",
        "    params.append({'clf__max_features': [None], 'clf__min_samples_split': [2], 'clf__min_samples_leaf':[1],\n",
        "                  'clf__class_weight': ['balanced']})\n",
        "    names.append('DecisionTreeClassifier') \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    return pipelines, params, names\n",
        "\n",
        "  def model(self, pipeline, param, name, X, y):    \n",
        "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=32)\n",
        "\n",
        "    grid_obj = GridSearchCV(estimator=pipeline, param_grid=param, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
        "    grid_obj.fit(X,y)  \n",
        "\n",
        "    print(name, 'F1-measure:', grid_obj.best_score_)\n",
        "    estimator = grid_obj.best_estimator_\n",
        "    estimator.fit(X,y) # train on all training dataset\n",
        "    return estimator \n",
        "\n",
        "\n",
        "  def estimators(self, pipelines, params, names,  X_train, y_train):\n",
        "    estimators = []\n",
        "    for idx in range(0,len(pipelines)):    \n",
        "        estimators.append(self.model(pipelines[idx], params[idx], names[idx], X_train, y_train))\n",
        "    return estimators\n",
        "\n",
        "\n",
        "  def evaluate_models(self,estimators, names, X_test, y_test):\n",
        "   \n",
        "    for idx, estimator in enumerate(estimators):\n",
        "      print('\\nPerformance of', names[idx])\n",
        "      y_pred = estimator.predict(X_test)       \n",
        "      print('\\nConfusion matrix\\n', confusion_matrix(y_test, y_pred), '\\n')    \n",
        "      print('F1-measure', f1_score(y_test, y_pred, average='micro'), '\\n')\n",
        "\n",
        "      precision, recall, fscore, support = score(y_test, y_pred)\n",
        "\n",
        "      print('precision: {}'.format(precision))\n",
        "      print('recall: {}'.format(recall))\n",
        "      print('fscore: {}'.format(fscore))\n",
        "      print('support: {}'.format(support))\n",
        "\n",
        "    return\n",
        "\n",
        "#script\n",
        "modeling = Modeling()\n",
        "\n",
        "X_train, y_train, X_test, y_test = modeling.split_train_test(Final_Data_Source, Final_Data_Target)\n",
        "X_test_T, y_test_T, X_test_T, y_test_T = modeling.split_train_test(Data_Target, Data_Target)\n",
        "X_train_S,  X_test_S, y_train_S, y_test_S = modeling.split_train_val(X_test_T, y_test_T                                                                  \n",
        "                                                                     )\n",
        "\n",
        "pipelines, params, names = modeling.pipelines_def()\n",
        "print('Source Training results_without TL')\n",
        "estimators = modeling.estimators(pipelines, params, names, X_train_S, y_train_S)\n",
        "print('Source Testing results_without TL')\n",
        "modeling.evaluate_models(estimators, names, X_test_S, y_test_S)\n",
        "\n",
        "\n",
        "print('Training results_TL')\n",
        "estimators = modeling.estimators(pipelines, params, names, X_train, y_train)\n",
        "print('Testing results_TL')\n",
        "modeling.evaluate_models(estimators, names, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Training results_without TL\n",
            "DecisionTreeClassifier F1-measure: 0.8521869835800431\n",
            "Source Testing results_without TL\n",
            "\n",
            "Performance of DecisionTreeClassifier\n",
            "\n",
            "Confusion matrix\n",
            " [[324  26   7  15   7]\n",
            " [  6 121   2   0   2]\n",
            " [ 23  15 274   0  65]\n",
            " [  4   0   0  94   0]\n",
            " [  2   0   5   0  32]] \n",
            "\n",
            "F1-measure 0.8251953125 \n",
            "\n",
            "precision: [0.90250696 0.74691358 0.95138889 0.86238532 0.30188679]\n",
            "recall: [0.85488127 0.92366412 0.72679045 0.95918367 0.82051282]\n",
            "fscore: [0.87804878 0.82593857 0.82406015 0.90821256 0.44137931]\n",
            "support: [379 131 377  98  39]\n",
            "Training results_TL\n",
            "DecisionTreeClassifier F1-measure: 0.9123273844039257\n",
            "Testing results_TL\n",
            "\n",
            "Performance of DecisionTreeClassifier\n",
            "\n",
            "Confusion matrix\n",
            " [[330  28   5  12   5]\n",
            " [  2 107   0   0   1]\n",
            " [ 13  15 321   0   4]\n",
            " [  5   0   0  89   0]\n",
            " [  1   0   7   0  27]] \n",
            "\n",
            "F1-measure 0.8991769547325102 \n",
            "\n",
            "precision: [0.94017094 0.71333333 0.96396396 0.88118812 0.72972973]\n",
            "recall: [0.86842105 0.97272727 0.90934844 0.94680851 0.77142857]\n",
            "fscore: [0.90287278 0.82307692 0.93586006 0.91282051 0.75      ]\n",
            "support: [380 110 353  94  35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQYTxpnvqAVE"
      },
      "source": [
        "#Thank you"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}